# models/encoders/hsi_encoder.py
"""
SOTAè½»é‡çº§é«˜å…‰è°±å›¾åƒç¼–ç å™¨ - ä¿®å¤ç‰ˆ
ä¿®å¤äº†ç»´åº¦ä¸åŒ¹é…é—®é¢˜
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from einops import rearrange, repeat
import math


class SpectralGroupAttention(nn.Module):
    """å…‰è°±åˆ†ç»„æ³¨æ„åŠ›"""
    def __init__(self, in_channels, num_groups=8, reduction=4):
        super().__init__()
        self.num_groups = num_groups
        self.group_channels = in_channels // num_groups
        
        self.group_attention = nn.ModuleList([
            nn.Sequential(
                nn.AdaptiveAvgPool2d(1),
                nn.Conv2d(self.group_channels, self.group_channels // reduction, 1),
                nn.ReLU(inplace=True),
                nn.Conv2d(self.group_channels // reduction, self.group_channels, 1),
                nn.Sigmoid()
            ) for _ in range(num_groups)
        ])
    
    def forward(self, x):
        B, C, H, W = x.shape
        x = x.view(B, self.num_groups, self.group_channels, H, W)
        
        out = []
        for i in range(self.num_groups):
            group = x[:, i]
            att = self.group_attention[i](group)
            out.append(group * att)
        
        out = torch.cat(out, dim=1)
        return out


class EfficientSpectralReduction(nn.Module):
    """é«˜æ•ˆå…‰è°±é™ç»´æ¨¡å—"""
    def __init__(self, in_channels, out_channels, num_groups=8):
        super().__init__()
        
        self.depthwise = nn.Conv2d(
            in_channels, in_channels,
            kernel_size=1, groups=in_channels, bias=False
        )
        self.bn1 = nn.BatchNorm2d(in_channels)
        
        self.spectral_att = SpectralGroupAttention(in_channels, num_groups)
        
        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_channels)
        self.act = nn.GELU()
        
    def forward(self, x):
        x = self.depthwise(x)
        x = self.bn1(x)
        x = self.spectral_att(x)
        x = self.pointwise(x)
        x = self.bn2(x)
        x = self.act(x)
        return x


class SpatialReductionBlock(nn.Module):
    """ç©ºé—´ä¸‹é‡‡æ ·å—"""
    def __init__(self, in_channels, out_channels, stride=2):
        super().__init__()
        self.conv = nn.Conv2d(
            in_channels, out_channels,
            kernel_size=3, stride=stride, padding=1, bias=False
        )
        self.bn = nn.BatchNorm2d(out_channels)
        self.act = nn.GELU()
        
    def forward(self, x):
        return self.act(self.bn(self.conv(x)))


class GroupedSpatialAttention(nn.Module):
    """
    åˆ†ç»„ç©ºé—´æ³¨æ„åŠ› - ä¿®å¤ç‰ˆ
    è‡ªåŠ¨è®¡ç®—åˆé€‚çš„æ³¨æ„åŠ›å¤´æ•°
    """
    def __init__(self, channels, num_heads=None):
        super().__init__()
        
        # ğŸ”§ ä¿®å¤ï¼šè‡ªåŠ¨é€‰æ‹©èƒ½æ•´é™¤çš„å¤´æ•°
        if num_heads is None:
            # ä¼˜å…ˆé€‰æ‹©çš„å¤´æ•°åˆ—è¡¨ï¼ˆä»å¤§åˆ°å°ï¼‰
            preferred_heads = [16, 12, 8, 6, 4, 2, 1]
            for h in preferred_heads:
                if channels % h == 0:
                    num_heads = h
                    break
        else:
            # å¦‚æœæŒ‡å®šäº†å¤´æ•°ä½†ä¸èƒ½æ•´é™¤ï¼Œè‡ªåŠ¨è°ƒæ•´
            if channels % num_heads != 0:
                print(f"Warning: channels({channels}) % num_heads({num_heads}) != 0")
                print(f"Auto-adjusting num_heads...")
                preferred_heads = [16, 12, 8, 6, 4, 2, 1]
                for h in preferred_heads:
                    if channels % h == 0 and h <= num_heads:
                        num_heads = h
                        break
                print(f"Adjusted num_heads to {num_heads}")
        
        self.num_heads = num_heads
        self.head_channels = channels // num_heads
        
        self.qkv = nn.Conv2d(channels, channels * 3, kernel_size=1, bias=False)
        self.proj = nn.Conv2d(channels, channels, kernel_size=1, bias=False)
        self.scale = self.head_channels ** -0.5
        
    def forward(self, x):
        B, C, H, W = x.shape
        
        # ç”ŸæˆQ, K, V
        qkv = self.qkv(x)
        q, k, v = torch.chunk(qkv, 3, dim=1)
        
        # é‡å¡‘ä¸ºå¤šå¤´
        q = rearrange(q, 'b (h c) x y -> b h (x y) c', h=self.num_heads)
        k = rearrange(k, 'b (h c) x y -> b h (x y) c', h=self.num_heads)
        v = rearrange(v, 'b (h c) x y -> b h (x y) c', h=self.num_heads)
        
        # æ³¨æ„åŠ›è®¡ç®—
        attn = (q @ k.transpose(-2, -1)) * self.scale
        attn = attn.softmax(dim=-1)
        
        # åº”ç”¨æ³¨æ„åŠ›
        out = attn @ v
        out = rearrange(out, 'b h (x y) c -> b (h c) x y', x=H, y=W)
        out = self.proj(out)
        
        return out


class SpectralSpatialBlock(nn.Module):
    """å…‰è°±-ç©ºé—´è”åˆå¤„ç†å—"""
    def __init__(self, channels, num_heads=None, mlp_ratio=2.0, dropout=0.0):
        super().__init__()
        
        self.norm1 = nn.LayerNorm(channels)
        self.spatial_attn = GroupedSpatialAttention(channels, num_heads)
        self.dropout1 = nn.Dropout(dropout)
        
        self.norm2 = nn.LayerNorm(channels)
        mlp_hidden = int(channels * mlp_ratio)
        self.mlp = nn.Sequential(
            nn.Linear(channels, mlp_hidden),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(mlp_hidden, channels),
            nn.Dropout(dropout)
        )
        
    def forward(self, x):
        B, C, H, W = x.shape
        
        # ç©ºé—´æ³¨æ„åŠ›
        identity = x
        x = rearrange(x, 'b c h w -> b (h w) c')
        x = self.norm1(x)
        x = rearrange(x, 'b (h w) c -> b c h w', h=H, w=W)
        x = self.spatial_attn(x)
        x = identity + self.dropout1(x)
        
        # MLP
        identity = x
        x = rearrange(x, 'b c h w -> b (h w) c')
        x = self.norm2(x)
        x = self.mlp(x)
        x = rearrange(x, 'b (h w) c -> b c h w', h=H, w=W)
        x = identity + x
        
        return x


class HSIEncoder(nn.Module):
    """
    SOTAè½»é‡çº§é«˜å…‰è°±ç¼–ç å™¨ - ä¿®å¤ç‰ˆ
    """
    
    def __init__(
        self,
        hsi_channels=224,
        spatial_size=64,
        embed_dim=768,
        num_groups=8,
        num_heads=8,  # è¿™ä¸ªå‚æ•°ä¼šè¢«è‡ªåŠ¨è°ƒæ•´
        depth=2,
        dropout=0.1,
        **kwargs
    ):
        super().__init__()
        
        self.hsi_channels = hsi_channels
        self.spatial_size = spatial_size
        self.embed_dim = embed_dim
        
        # Stage 1: å…‰è°±é™ç»´ 224 -> 64
        self.spectral_reduction = EfficientSpectralReduction(
            in_channels=hsi_channels,
            out_channels=64,
            num_groups=num_groups
        )
        
        # Stage 2: ç©ºé—´ä¸‹é‡‡æ · 64x64 -> 32x32
        self.spatial_down1 = SpatialReductionBlock(64, 128, stride=2)
        
        # Stage 3: å…‰è°±-ç©ºé—´è”åˆå¤„ç†
        # ğŸ”§ ä¿®å¤ï¼š128é€šé“ï¼Œè‡ªåŠ¨é€‰æ‹©num_heads=8ï¼ˆ128èƒ½è¢«8æ•´é™¤ï¼‰
        self.ss_block1 = SpectralSpatialBlock(128, num_heads=8, dropout=dropout)
        
        # Stage 4: è¿›ä¸€æ­¥ç©ºé—´ä¸‹é‡‡æ · 32x32 -> 16x16
        self.spatial_down2 = SpatialReductionBlock(128, 256, stride=2)
        
        # Stage 5: æ·±å±‚ç‰¹å¾æå–
        # ğŸ”§ ä¿®å¤ï¼š256é€šé“ï¼Œè‡ªåŠ¨é€‰æ‹©num_heads=8ï¼ˆ256èƒ½è¢«8æ•´é™¤ï¼‰
        self.ss_blocks = nn.ModuleList([
            SpectralSpatialBlock(256, num_heads=8, dropout=dropout)
            for _ in range(depth)
        ])
        
        # Stage 6: ç‰¹å¾èšåˆ
        self.global_pool = nn.AdaptiveAvgPool2d(1)
        
        # æœ€ç»ˆæŠ•å½±åˆ°embed_dim
        self.final_proj = nn.Sequential(
            nn.Linear(256, embed_dim),
            nn.LayerNorm(embed_dim),
            nn.GELU(),
            nn.Dropout(dropout)
        )
        
        # Patch projection
        self.patch_size = 4
        num_patches = (16 // self.patch_size) ** 2
        
        self.patch_proj = nn.Conv2d(256, embed_dim, kernel_size=self.patch_size, stride=self.patch_size)
        self.patch_norm = nn.LayerNorm(embed_dim)
        
        # Position embedding
        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))
        nn.init.trunc_normal_(self.pos_embed, std=0.02)
        
        self._initialize_weights()
        
        # æ‰“å°æ¨¡å‹ä¿¡æ¯
        print(f"âœ“ HSI Encoder initialized:")
        print(f"  - Stage 3: 128 channels, 8 heads")
        print(f"  - Stage 5: 256 channels, 8 heads")
    
    def _initialize_weights(self):
        """åˆå§‹åŒ–æƒé‡"""
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                nn.init.trunc_normal_(m.weight, std=0.02)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
    
    def forward(self, x):
        """
        Args:
            x: [B, C, H, W] é«˜å…‰è°±å›¾åƒ
            
        Returns:
            cls_token: [B, embed_dim]
            patch_tokens: [B, num_patches, embed_dim]
        """
        B = x.shape[0]
        
        # Stage 1: å…‰è°±é™ç»´
        x = self.spectral_reduction(x)
        
        # Stage 2: ç©ºé—´ä¸‹é‡‡æ ·1
        x = self.spatial_down1(x)
        
        # Stage 3: å…‰è°±-ç©ºé—´å¤„ç†
        x = self.ss_block1(x)
        
        # Stage 4: ç©ºé—´ä¸‹é‡‡æ ·2
        x = self.spatial_down2(x)
        
        # Stage 5: æ·±å±‚ç‰¹å¾æå–
        for block in self.ss_blocks:
            x = block(x)
        
        # ç”Ÿæˆå…¨å±€ç‰¹å¾
        global_feat = self.global_pool(x).flatten(1)
        cls_token = self.final_proj(global_feat)
        
        # ç”Ÿæˆpatch tokens
        patch_feat = self.patch_proj(x)
        patch_tokens = rearrange(patch_feat, 'b c h w -> b (h w) c')
        patch_tokens = self.patch_norm(patch_tokens)
        patch_tokens = patch_tokens + self.pos_embed
        
        return cls_token, patch_tokens
    
    def get_feature_maps(self, x):
        """è¿”å›ä¸­é—´ç‰¹å¾å›¾"""
        features = {}
        
        x = self.spectral_reduction(x)
        features['stage1'] = x
        
        x = self.spatial_down1(x)
        features['stage2'] = x
        
        x = self.ss_block1(x)
        features['stage3'] = x
        
        x = self.spatial_down2(x)
        features['stage4'] = x
        
        for i, block in enumerate(self.ss_blocks):
            x = block(x)
            features[f'stage5_{i}'] = x
        
        return features


# å†…å­˜ä¼˜åŒ–ç‰ˆæœ¬
class HSIEncoderCheckpoint(HSIEncoder):
    """å¸¦æ¢¯åº¦æ£€æŸ¥ç‚¹çš„HSIç¼–ç å™¨"""
    def forward(self, x):
        B = x.shape[0]
        
        from torch.utils.checkpoint import checkpoint
        
        x = checkpoint(self.spectral_reduction, x, use_reentrant=False)
        x = checkpoint(self.spatial_down1, x, use_reentrant=False)
        x = checkpoint(self.ss_block1, x, use_reentrant=False)
        x = checkpoint(self.spatial_down2, x, use_reentrant=False)
        
        for block in self.ss_blocks:
            x = checkpoint(block, x, use_reentrant=False)
        
        global_feat = self.global_pool(x).flatten(1)
        cls_token = self.final_proj(global_feat)
        
        patch_feat = self.patch_proj(x)
        patch_tokens = rearrange(patch_feat, 'b c h w -> b (h w) c')
        patch_tokens = self.patch_norm(patch_tokens)
        patch_tokens = patch_tokens + self.pos_embed
        
        return cls_token, patch_tokens


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    print("="*60)
    print("æµ‹è¯•ä¿®å¤åçš„HSIç¼–ç å™¨")
    print("="*60)
    
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    
    # åˆ›å»ºæ¨¡å‹
    model = HSIEncoder(
        hsi_channels=224,
        spatial_size=64,
        embed_dim=768,
        num_groups=8,
        num_heads=8,  # ä¼šè¢«è‡ªåŠ¨è°ƒæ•´
        depth=2
    ).to(device)
    
    # æµ‹è¯•ä¸åŒbatch size
    print("\næ˜¾å­˜æµ‹è¯•:")
    print("-"*60)
    
    for batch_size in [1, 4, 8, 16, 20]:
        try:
            x = torch.randn(batch_size, 224, 64, 64).to(device)
            
            with torch.cuda.amp.autocast():
                cls_token, patch_tokens = model(x)
            
            if device == 'cuda':
                mem = torch.cuda.memory_allocated() / 1024**3
                print(f"Batch {batch_size:2d}: cls={cls_token.shape}, "
                      f"patches={patch_tokens.shape}, æ˜¾å­˜={mem:.2f}GB")
            else:
                print(f"Batch {batch_size:2d}: cls={cls_token.shape}, "
                      f"patches={patch_tokens.shape}")
            
            del x, cls_token, patch_tokens
            if device == 'cuda':
                torch.cuda.empty_cache()
            
        except RuntimeError as e:
            if 'out of memory' in str(e):
                print(f"Batch {batch_size:2d}: OOM")
                break
            else:
                raise e
    
    print("\n" + "="*60)
    print("âœ“ æµ‹è¯•å®Œæˆï¼")
    print("="*60)