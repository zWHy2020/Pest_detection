# scripts/train_qwen_fixed.py
"""
ä¿®å¤ç‰ˆQwenè®­ç»ƒè„šæœ¬ - è§£å†³æ˜¾å­˜ä¸è¶³é—®é¢˜
å…³é”®ä¼˜åŒ–ï¼š
1. æ¢¯åº¦ç´¯ç§¯
2. æ›´æ¿€è¿›çš„æ˜¾å­˜ç®¡ç†
3. æ¢¯åº¦æ£€æŸ¥ç‚¹
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.cuda.amp import autocast, GradScaler
from torch.utils.tensorboard import SummaryWriter
import argparse
import os
import sys
from tqdm import tqdm
import json
import numpy as np
import warnings
import gc
warnings.filterwarnings('ignore')

PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, PROJECT_ROOT)

from models.main_model_qwen import MultiModalPestDetectionWithQwen
from data import create_dataloaders
from utils import calculate_metrics, AverageMeter, plot_training_curves


class MemoryEfficientQwenTrainer:
    """æ˜¾å­˜ä¼˜åŒ–çš„Qwenè®­ç»ƒå™¨"""
    
    def __init__(self, args):
        self.args = args
        self.setup_device()
        self.setup_dataloader()
        self.setup_model()
        self.setup_optimizer()
        self.setup_training()
    
    def setup_device(self):
        """è®¾ç½®è®¾å¤‡"""
        if not torch.cuda.is_available():
            raise RuntimeError("éœ€è¦GPU")
        
        self.device = torch.device('cuda:0')
        
        print("\n" + "="*60)
        print("æ˜¾å­˜ä¼˜åŒ–çš„Qwenè®­ç»ƒå™¨")
        print("="*60)
        print(f"è®¾å¤‡: {self.device}")
        
        # ğŸ”§ æ¸…ç†æ˜¾å­˜
        torch.cuda.empty_cache()
        gc.collect()
        
        # ğŸ”§ æ˜¾å­˜ä¼˜åŒ–è®¾ç½®
        torch.backends.cudnn.benchmark = True
        torch.backends.cuda.matmul.allow_tf32 = True
        
        # æ˜¾ç¤ºæ˜¾å­˜çŠ¶æ€
        total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        print(f"æ€»æ˜¾å­˜: {total_memory:.2f} GB")
    
    def setup_dataloader(self):
        """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
        print("\nåˆ›å»ºæ•°æ®åŠ è½½å™¨...")
        
        # ğŸ”§ å‡å°‘workersä»¥èŠ‚çœå†…å­˜
        self.train_loader, self.val_loader, _ = create_dataloaders(
            data_root=self.args.data_root,
            batch_size=self.args.batch_size,
            num_workers=min(self.args.num_workers, 2),  # æœ€å¤š2ä¸ªworker
            text_model_name='bert-base-chinese',
            use_hsi=self.args.use_hsi,
            use_augmentation=False  # Qwenè®­ç»ƒæ—¶ä¸ç”¨å¢å¼º
        )
        
        self.num_classes = self.train_loader.dataset.num_classes
        
        print(f"âœ“ è®­ç»ƒé›†: {len(self.train_loader.dataset)} æ ·æœ¬")
        print(f"âœ“ éªŒè¯é›†: {len(self.val_loader.dataset)} æ ·æœ¬")
        print(f"âœ“ ç±»åˆ«æ•°: {self.num_classes}")
        print(f"âœ“ Batch size: {self.args.batch_size}")
        print(f"âœ“ æ¢¯åº¦ç´¯ç§¯æ­¥æ•°: {self.args.accumulation_steps}")
        print(f"âœ“ æœ‰æ•ˆbatch size: {self.args.batch_size * self.args.accumulation_steps}")
    
    def setup_model(self):
        """åˆ›å»ºæ¨¡å‹"""
        print("\nåˆ›å»ºQwenå¢å¼ºæ¨¡å‹...")
        
        self.model = MultiModalPestDetectionWithQwen(
            num_classes=self.num_classes,
            rgb_image_size=224,
            hsi_image_size=64,
            hsi_channels=224,
            text_model_name='bert-base-chinese',
            embed_dim=768,
            num_heads=12,
            dropout=0.1,
            fusion_layers=4,
            fusion_strategy='hierarchical',
            qwen_path=self.args.qwen_path,
            use_hsi=self.args.use_hsi,
            freeze_encoders=False,
            use_lora=True,
            lora_rank=8,
            lora_alpha=16,
            num_query_tokens=32
        )

        # ç§»åˆ°è®¾å¤‡
        self.model = self.model.to(self.device)
        
        print("âœ“ æ¨¡å‹åˆ›å»ºå®Œæˆ")
        
        # æ˜¾ç¤ºæ˜¾å­˜ä½¿ç”¨
        allocated = torch.cuda.memory_allocated(0) / 1024**3
        reserved = torch.cuda.memory_reserved(0) / 1024**3
        print(f"æ¨¡å‹åŠ è½½åæ˜¾å­˜: å·²åˆ†é… {allocated:.2f}GB, å·²ä¿ç•™ {reserved:.2f}GB")
    
    def setup_optimizer(self):
        """è®¾ç½®ä¼˜åŒ–å™¨"""
        print("\nè®¾ç½®ä¼˜åŒ–å™¨...")
        
        # åˆ†ç»„å‚æ•° - ä¸åŒæ¨¡å—ä¸åŒå­¦ä¹ ç‡
        encoder_params = []
        fusion_params = []
        adapter_params = []
        qwen_params = []
        classifier_params = []
        
        for name, param in self.model.named_parameters():
            if not param.requires_grad:
                continue
            
            if 'encoder' in name:
                encoder_params.append(param)
            elif 'fusion' in name or 'alignment' in name:
                fusion_params.append(param)
            elif 'embedding_adapter' in name:
                adapter_params.append(param)
            elif 'qwen' in name:
                qwen_params.append(param)
            elif 'classifier' in name:
                classifier_params.append(param)
        
        param_groups = [
            {'params': encoder_params, 'lr': self.args.lr * 0.1},
            {'params': fusion_params, 'lr': self.args.lr * 0.5},
            {'params': adapter_params, 'lr': self.args.lr},
            {'params': qwen_params, 'lr': self.args.lr * 0.1},
            {'params': classifier_params, 'lr': self.args.lr}
        ]
        
        self.optimizer = optim.AdamW(
            param_groups,
            lr=self.args.lr,
            weight_decay=0.01,
            betas=(0.9, 0.999)
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(
            self.optimizer,
            T_0=10,
            T_mult=2,
            eta_min=self.args.lr * 0.01
        )
        
        print("âœ“ ä¼˜åŒ–å™¨è®¾ç½®å®Œæˆ")
    
    def setup_training(self):
        """è®¾ç½®è®­ç»ƒç»„ä»¶"""
        # æ··åˆç²¾åº¦
        self.scaler = GradScaler() if self.args.use_amp else None
        
        # TensorBoard
        log_dir = os.path.join(self.args.output_dir, 'logs')
        os.makedirs(log_dir, exist_ok=True)
        self.writer = SummaryWriter(log_dir)
        
        # æ£€æŸ¥ç‚¹ç›®å½•
        self.checkpoint_dir = os.path.join(self.args.output_dir, 'checkpoints')
        os.makedirs(self.checkpoint_dir, exist_ok=True)
        
        self.best_val_acc = 0.0
        self.best_epoch = 0
        self.global_step = 0
        
        self.history = {
            'train_loss': [],
            'val_loss': [],
            'val_acc': [],
            'val_f1': []
        }
    
    def train_epoch(self, epoch):
        """è®­ç»ƒä¸€ä¸ªepoch - å¸¦æ¢¯åº¦ç´¯ç§¯å’Œæ˜¾å­˜ä¼˜åŒ–"""
        self.model.train()
        
        losses = AverageMeter()
        cls_losses = AverageMeter()
        
        pbar = tqdm(self.train_loader, desc=f'Epoch {epoch}', ncols=100)
        
        # ğŸ”§ æ¢¯åº¦ç´¯ç§¯
        self.optimizer.zero_grad()
        
        for batch_idx, batch in enumerate(pbar):
            try:
                # ç§»åŠ¨æ•°æ®
                rgb = batch['rgb_images'].to(self.device, non_blocking=True)
                hsi = batch['hsi_images'].to(self.device, non_blocking=True)
                text_ids = batch['text_input_ids'].to(self.device, non_blocking=True)
                text_mask = batch['text_attention_mask'].to(self.device, non_blocking=True)
                labels = batch['labels'].to(self.device, non_blocking=True)
                
                # ğŸ”§ ä½¿ç”¨æ··åˆç²¾åº¦
                with autocast(dtype=torch.float16):
                    outputs = self.model(rgb, hsi, text_ids, text_mask, labels=labels)
                    loss = outputs['total_loss']
                    
                    if isinstance(loss, tuple):
                        loss = loss[0]
                    loss = loss.mean()
                    
                    # ğŸ”§ æ¢¯åº¦ç´¯ç§¯ï¼šæŸå¤±é™¤ä»¥ç´¯ç§¯æ­¥æ•°
                    loss = loss / self.args.accumulation_steps
                
                # åå‘ä¼ æ’­
                self.scaler.scale(loss).backward()
                
                # ğŸ”§ æ¯accumulation_stepsæ­¥æ›´æ–°ä¸€æ¬¡
                if (batch_idx + 1) % self.args.accumulation_steps == 0:
                    # æ¢¯åº¦è£å‰ª
                    if self.args.clip_grad > 0:
                        self.scaler.unscale_(self.optimizer)
                        torch.nn.utils.clip_grad_norm_(
                            self.model.parameters(), self.args.clip_grad
                        )
                    
                    # æ›´æ–°å‚æ•°
                    self.scaler.step(self.optimizer)
                    self.scaler.update()
                    self.optimizer.zero_grad()
                    
                    self.global_step += 1
                
                # ç»Ÿè®¡ï¼ˆè®°å½•çœŸå®lossï¼Œä¸æ˜¯é™¤ä»¥accumulation_stepsåçš„ï¼‰
                real_loss = loss.item() * self.args.accumulation_steps
                losses.update(real_loss, rgb.size(0))
                
                cls_loss = outputs.get('cls_loss', loss)
                if isinstance(cls_loss, tuple):
                    cls_loss = cls_loss[0]
                cls_losses.update(cls_loss.mean().item(), rgb.size(0))
                
                # æ›´æ–°è¿›åº¦æ¡
                pbar.set_postfix({
                    'loss': f'{losses.avg:.3f}',
                    'cls': f'{cls_losses.avg:.3f}'
                })
                
                # ğŸ”§ å®šæœŸæ¸…ç†æ˜¾å­˜
                if batch_idx % 50 == 0:
                    torch.cuda.empty_cache()
                    
                    # TensorBoard
                    self.writer.add_scalar('Train/Loss', losses.avg, self.global_step)
                    self.writer.add_scalar('Train/ClsLoss', cls_losses.avg, self.global_step)
                
                # ğŸ”§ æ˜¾ç¤ºæ˜¾å­˜ä½¿ç”¨ï¼ˆå‰å‡ ä¸ªbatchï¼‰
                if epoch == 1 and batch_idx < 5:
                    allocated = torch.cuda.memory_allocated(0) / 1024**3
                    reserved = torch.cuda.memory_reserved(0) / 1024**3
                    print(f"\nBatch {batch_idx}: æ˜¾å­˜ {allocated:.2f}/{reserved:.2f}GB")
                
            except RuntimeError as e:
                if 'out of memory' in str(e):
                    print(f'\nâŒ Batch {batch_idx} æ˜¾å­˜ä¸è¶³!')
                    print(f'å½“å‰batch_size: {self.args.batch_size}')
                    print(f'å»ºè®®å‡å°åˆ°: {self.args.batch_size // 2}')
                    
                    # æ¸…ç†æ˜¾å­˜
                    torch.cuda.empty_cache()
                    gc.collect()
                    raise e
                else:
                    raise e
        
        # ğŸ”§ epochç»“æŸæ¸…ç†
        torch.cuda.empty_cache()
        gc.collect()
        
        return losses.avg
    
    @torch.no_grad()
    def validate(self, epoch):
        """éªŒè¯ - æ˜¾å­˜ä¼˜åŒ–"""
        self.model.eval()
        
        losses = AverageMeter()
        all_preds = []
        all_labels = []
        
        pbar = tqdm(self.val_loader, desc=f'Val {epoch}', ncols=100)
        
        for batch in pbar:
            try:
                rgb = batch['rgb_images'].to(self.device, non_blocking=True)
                hsi = batch['hsi_images'].to(self.device, non_blocking=True)
                text_ids = batch['text_input_ids'].to(self.device, non_blocking=True)
                text_mask = batch['text_attention_mask'].to(self.device, non_blocking=True)
                labels = batch['labels'].to(self.device, non_blocking=True)
                
                # ğŸ”§ éªŒè¯æ—¶ä¹Ÿç”¨æ··åˆç²¾åº¦
                with autocast(dtype=torch.float16):
                    outputs = self.model(rgb, hsi, text_ids, text_mask, labels=labels)
                
                loss = outputs['total_loss']
                if isinstance(loss, tuple):
                    loss = loss[0]
                loss = loss.mean()
                
                logits = outputs['logits']
                preds = torch.argmax(logits, dim=1)
                
                losses.update(loss.item(), rgb.size(0))
                all_preds.extend(preds.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())
                
                pbar.set_postfix({'loss': f'{losses.avg:.3f}'})
                
            except Exception as e:
                print(f'\nâŒ éªŒè¯é”™è¯¯: {e}')
                continue
        
        # ğŸ”§ éªŒè¯ç»“æŸæ¸…ç†
        torch.cuda.empty_cache()
        gc.collect()
        
        # è®¡ç®—æŒ‡æ ‡
        metrics = calculate_metrics(
            np.array(all_labels),
            np.array(all_preds),
            num_classes=self.num_classes
        )
        
        return losses.avg, metrics['accuracy'], metrics['f1']
    
    def save_checkpoint(self, epoch, val_acc, is_best=False):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'val_acc': val_acc,
            'best_val_acc': self.best_val_acc
        }
        
        # ä¿å­˜æœ€æ–°
        latest_path = os.path.join(self.checkpoint_dir, 'latest.pth')
        torch.save(checkpoint, latest_path)
        
        # ä¿å­˜æœ€ä½³
        if is_best:
            best_path = os.path.join(self.checkpoint_dir, 'best_model.pth')
            torch.save(checkpoint, best_path)
            print(f"   âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Acc: {val_acc:.4f})")
    
    def train(self):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        print("\n" + "="*60)
        print("å¼€å§‹Qwenè®­ç»ƒ")
        print("="*60)
        
        for epoch in range(1, self.args.epochs + 1):
            print(f"\nEpoch {epoch}/{self.args.epochs}")
            print("-" * 60)
            
            try:
                # è®­ç»ƒ
                train_loss = self.train_epoch(epoch)
                
                # éªŒè¯
                val_loss, val_acc, val_f1 = self.validate(epoch)
                
                # å­¦ä¹ ç‡è°ƒåº¦
                self.scheduler.step()
                
                # è®°å½•
                self.history['train_loss'].append(train_loss)
                self.history['val_loss'].append(val_loss)
                self.history['val_acc'].append(val_acc)
                self.history['val_f1'].append(val_f1)
                
                # TensorBoard
                self.writer.add_scalar('Val/Loss', val_loss, epoch)
                self.writer.add_scalar('Val/Accuracy', val_acc, epoch)
                self.writer.add_scalar('Val/F1', val_f1, epoch)
                
                # æ‰“å°
                print(f"\nç»“æœ:")
                print(f"  Train Loss: {train_loss:.4f}")
                print(f"  Val Loss: {val_loss:.4f}")
                print(f"  Val Acc: {val_acc:.4f}")
                print(f"  Val F1: {val_f1:.4f}")
                
                # æ˜¾å­˜çŠ¶æ€
                allocated = torch.cuda.memory_allocated(0) / 1024**3
                reserved = torch.cuda.memory_reserved(0) / 1024**3
                print(f"  æ˜¾å­˜: {allocated:.2f}/{reserved:.2f}GB")
                
                # ä¿å­˜
                is_best = val_acc > self.best_val_acc
                if is_best:
                    self.best_val_acc = val_acc
                    self.best_epoch = epoch
                
                self.save_checkpoint(epoch, val_acc, is_best)
                
                # æ—©åœ
                if epoch - self.best_epoch >= self.args.early_stopping:
                    print(f"\næ—©åœè§¦å‘ (æœ€ä½³epoch: {self.best_epoch})")
                    break
                
            except Exception as e:
                print(f"\nâŒ Epoch {epoch} å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                break
        
        # ä¿å­˜å†å²
        history_path = os.path.join(self.args.output_dir, 'history.json')
        with open(history_path, 'w') as f:
            json.dump(self.history, f, indent=2)
        
        # ç»˜åˆ¶æ›²çº¿
        try:
            curves_path = os.path.join(self.args.output_dir, 'curves.png')
            plot_training_curves(self.history, save_path=curves_path)
        except:
            pass
        
        print("\n" + "="*60)
        print("âœ“ è®­ç»ƒå®Œæˆ!")
        print("="*60)
        print(f"æœ€ä½³å‡†ç¡®ç‡: {self.best_val_acc:.4f} (Epoch {self.best_epoch})")
        print(f"è¾“å‡ºç›®å½•: {self.args.output_dir}")
        print("="*60)
        
        self.writer.close()


def parse_args():
    parser = argparse.ArgumentParser(description='æ˜¾å­˜ä¼˜åŒ–çš„Qwenè®­ç»ƒè„šæœ¬')
    
    # æ•°æ®
    parser.add_argument('--data_root', type=str, required=True)
    parser.add_argument('--batch_size', type=int, default=2,
                       help='æ¯ä¸ªGPUçš„batch sizeï¼ˆé»˜è®¤2ï¼‰')
    parser.add_argument('--num_workers', type=int, default=2,
                       help='æ•°æ®åŠ è½½çº¿ç¨‹æ•°ï¼ˆé»˜è®¤2ï¼‰')
    
    # è®­ç»ƒ
    parser.add_argument('--epochs', type=int, default=50)
    parser.add_argument('--lr', type=float, default=2e-5)
    parser.add_argument('--clip_grad', type=float, default=1.0)
    parser.add_argument('--use_amp', action='store_true',
                       help='ä½¿ç”¨æ··åˆç²¾åº¦ï¼ˆå¿…é¡»å¯ç”¨ï¼‰')
    parser.add_argument('--accumulation_steps', type=int, default=4,
                       help='æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ï¼ˆé»˜è®¤4ï¼Œæœ‰æ•ˆbatch=2*4=8ï¼‰')
    parser.add_argument('--early_stopping', type=int, default=10)
    
    # Qwen
    parser.add_argument('--qwen_path', type=str, required=True)
    
    # è¾“å‡º
    parser.add_argument('--output_dir', type=str, default='./outputs/qwen_fixed')
    parser.add_argument('--use_hsi', action=argparse.BooleanOptionalAction, default=True)
    
    return parser.parse_args()


def main():
    args = parse_args()
    
    # ğŸ”§ å¼ºåˆ¶å¯ç”¨æ··åˆç²¾åº¦
    if not args.use_amp:
        print("è­¦å‘Š: è‡ªåŠ¨å¯ç”¨æ··åˆç²¾åº¦ä»¥èŠ‚çœæ˜¾å­˜")
        args.use_amp = True
    
    os.makedirs(args.output_dir, exist_ok=True)
    
    torch.manual_seed(42)
    np.random.seed(42)
    
    print("="*60)
    print("  æ˜¾å­˜ä¼˜åŒ–çš„Qwenè®­ç»ƒè„šæœ¬")
    print("="*60)
    print(f"Batch size: {args.batch_size}")
    print(f"æ¢¯åº¦ç´¯ç§¯: {args.accumulation_steps}")
    print(f"æœ‰æ•ˆbatch: {args.batch_size * args.accumulation_steps}")
    print("="*60)
    
    try:
        trainer = MemoryEfficientQwenTrainer(args)
        trainer.train()
        
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == '__main__':
    main()